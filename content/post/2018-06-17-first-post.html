---
title: My First Post
author: John Lee
date: '2018-06-17'
slug: first-post
categories: []
tags: [linear regression, prediction]
subtitle: ''
---



<p>This is the first blog post of my life! I will be using this blog to post about anything that I want to share in statistics. For starter, I will run a linear regression with the <code>iris</code> dataset.</p>
<pre class="r"><code>names(iris)</code></pre>
<pre><code>## [1] &quot;Sepal.Length&quot; &quot;Sepal.Width&quot;  &quot;Petal.Length&quot; &quot;Petal.Width&quot; 
## [5] &quot;Species&quot;</code></pre>
<p>Let’s predict <code>Sepal.Length</code> with <code>Petal.Length</code> and <code>Petal.Width</code>.</p>
<pre class="r"><code>#separate into training and testing sets
set.seed(1234)
train_ind &lt;- sample(nrow(iris), floor(0.8 * nrow(iris)))
iris_train &lt;- iris[train_ind,]
iris_test &lt;- iris[-train_ind,]

#run linear regression
iris_lm &lt;- lm(Sepal.Length ~ Petal.Length + Petal.Width, data = iris_train)
summary(iris_lm)</code></pre>
<pre><code>## 
## Call:
## lm(formula = Sepal.Length ~ Petal.Length + Petal.Width, data = iris_train)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -1.18143 -0.30856 -0.03082  0.31355  1.01589 
## 
## Coefficients:
##              Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)   4.23175    0.11943  35.434  &lt; 2e-16 ***
## Petal.Length  0.49919    0.08618   5.792 5.96e-08 ***
## Petal.Width  -0.23334    0.19251  -1.212    0.228    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.4096 on 117 degrees of freedom
## Multiple R-squared:  0.7447, Adjusted R-squared:  0.7404 
## F-statistic: 170.7 on 2 and 117 DF,  p-value: &lt; 2.2e-16</code></pre>
<p>The corresponding equation for <code>iris_lm</code> model is <span class="math display">\[\hat{y}_i = 4.23 + 0.5x_{1i}  -0.23x_{2i}\]</span> Although all the coefficients show significance in terms of p-value, let’s create diagnostic plots to check if the model fits correctly.</p>
<pre class="r"><code>par(mfrow = c(2, 2))
plot(iris_lm)</code></pre>
<p><img src="/post/2018-06-17-first-post_files/figure-html/unnamed-chunk-3-1.png" width="672" /></p>
<p>There seems to be a nonlinear trend in the <code>Residuals vs Fitted</code> diagnostic plot. This suggests that the true model may not be linear. As a result, I will include a quadratic term for the predictors.</p>
<p>The resulting quadratic model will take the form of <span class="math display">\[\hat{y}_i = \hat{\beta}_0 + \hat{\beta}_{11}x_{1i} + \hat{\beta}_{12}x_{1i}^2 + \hat{\beta}_{21}x_{2i} + \hat{\beta}_{22}x_{2i}^2\]</span></p>
<pre class="r"><code>iris_lm_quad &lt;- lm(Sepal.Length ~ poly(Petal.Length, 2, raw = TRUE) + poly(Petal.Width, 2, raw = TRUE),
                   data = iris_train)
par(mfrow = c(2, 2))
plot(iris_lm_quad)</code></pre>
<p><img src="/post/2018-06-17-first-post_files/figure-html/unnamed-chunk-4-1.png" width="672" /></p>
<p>Now the non-linearity problem is gone! It looks as though there is an observation with leverage that is noticeably higher than that of others, but the observation has the Cook’s Distance well below 0.5; so I will not investigate further.</p>
<pre class="r"><code>summary(iris_lm_quad)</code></pre>
<pre><code>## 
## Call:
## lm(formula = Sepal.Length ~ poly(Petal.Length, 2, raw = TRUE) + 
##     poly(Petal.Width, 2, raw = TRUE), data = iris_train)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -0.99043 -0.24766 -0.03761  0.22157  0.82690 
## 
## Coefficients:
##                                    Estimate Std. Error t value Pr(&gt;|t|)
## (Intercept)                         5.10858    0.26741  19.104  &lt; 2e-16
## poly(Petal.Length, 2, raw = TRUE)1 -0.23246    0.29297  -0.793 0.429144
## poly(Petal.Length, 2, raw = TRUE)2  0.10776    0.03024   3.564 0.000533
## poly(Petal.Width, 2, raw = TRUE)1  -0.03862    0.58957  -0.066 0.947889
## poly(Petal.Width, 2, raw = TRUE)2  -0.09989    0.16066  -0.622 0.535341
##                                       
## (Intercept)                        ***
## poly(Petal.Length, 2, raw = TRUE)1    
## poly(Petal.Length, 2, raw = TRUE)2 ***
## poly(Petal.Width, 2, raw = TRUE)1     
## poly(Petal.Width, 2, raw = TRUE)2     
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.3651 on 115 degrees of freedom
## Multiple R-squared:  0.8006, Adjusted R-squared:  0.7937 
## F-statistic: 115.4 on 4 and 115 DF,  p-value: &lt; 2.2e-16</code></pre>
<p>Judging from the summary, it is evident that the quadratic polynomial for <code>Petal.Width</code> is not very significant. Let’s remove that predictor and see how the model turns out.</p>
<pre class="r"><code>iris_lm_quad &lt;- update(iris_lm_quad, Sepal.Length ~ poly(Petal.Length, 2, raw = TRUE) + Petal.Width)
summary(iris_lm_quad)</code></pre>
<pre><code>## 
## Call:
## lm(formula = Sepal.Length ~ poly(Petal.Length, 2, raw = TRUE) + 
##     Petal.Width, data = iris_train)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -0.97391 -0.27949 -0.04329  0.21732  0.84691 
## 
## Coefficients:
##                                    Estimate Std. Error t value Pr(&gt;|t|)
## (Intercept)                         4.98030    0.16967  29.353  &lt; 2e-16
## poly(Petal.Length, 2, raw = TRUE)1 -0.06818    0.12622  -0.540   0.5901
## poly(Petal.Length, 2, raw = TRUE)2  0.09193    0.01625   5.657 1.13e-07
## Petal.Width                        -0.38888    0.17336  -2.243   0.0268
##                                       
## (Intercept)                        ***
## poly(Petal.Length, 2, raw = TRUE)1    
## poly(Petal.Length, 2, raw = TRUE)2 ***
## Petal.Width                        *  
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.3642 on 116 degrees of freedom
## Multiple R-squared:  0.7999, Adjusted R-squared:  0.7948 
## F-statistic: 154.6 on 3 and 116 DF,  p-value: &lt; 2.2e-16</code></pre>
<p>Now all the predictors are significant with the exception for the first order polynomial for <code>Petal.Length</code>. Nonetheless, I have to respect the hierarchy of higher order terms, so I’ll keep the coefficient.</p>
<p>The summary suggests that the model is statistically significant, and has the R-squared value of roughly 0.8. I’ll use this model to predict the <code>Sepal.Length</code> with the test dataset created in the beginning of this post.</p>
<pre class="r"><code>y &lt;- iris_test[,&quot;Sepal.Length&quot;]
y_hat &lt;- predict(iris_lm_quad, iris_test[,c(&quot;Petal.Length&quot;, &quot;Petal.Width&quot;)])

#calculate Root Mean Squared Error
rmse &lt;- sqrt(sum((y - y_hat)^2)/length(y))</code></pre>
<p>I used Root Mean Square Error to quantify the accuracy of the model, and it came out to be 0.352.</p>
<p>This marks the end of my first post. As I continue this blog, I hope to learn statistics and the models more in depth and become more adept at coming up with creative solutions for data analysis problems!</p>
